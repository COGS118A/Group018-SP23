{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group members\n",
    "- Michael Nodini\n",
    "- Alex Cagle\n",
    "- Saransh Malik\n",
    "- Arthur Hewig\n",
    "- Maryam Usman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Our goal for this project is to develop a deep learning / computer vision model to identify breast cancer based on pathology images from the PatchCamelyon (PCam) dataset, using binary classification. The PatchCamelyon dataset consists of 327,680 color images of lymph node samples, each with a size of 96x96 pixels. The images are labeled with one of two classes either indicating the presence or absence of metastatic tissue (which indicates cancerous tissue). We will first pre-process the data to normalise the pixel values. Then, we will be training the last layer of a convolutional neural network on this data. The neural network will be trained to classify the images to a binary label (the detection of cancerous tissue). Success will be measured using sensitivity and specificity. Our main goal is to reduce the false negative rate (by maximizing sensitivity), since false negatives are far more fatal than false positives in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The article Artificial intelligence and computational pathology [Nature], published in 2021 by researchers Miao Cui and David Y. Zhang, discusses the immense potential that artificial intelligence has in the field of clinical pathology. Computational pathology uses technology to improve patient care in pathology and lab medicine. It combines different types of medical data, like images and patient information, to make better diagnoses and treatment plans. However, there are many challenges to overcome, like integrating the data and making sure the technology is used ethically. By utilizing modern computational methods, such as genomics, bioinformatics, and machine learning, and applying them to pathology, computational pathology has the potential to “create personalized diagnosis and treatment plans for patients,” “improve clinical workflow efficiency, diagnostic quality,” and offer a “better-integrated solution to whole-slide images, multi-omics data, and clinical informatics.” Most importantly, computational pathology can “reduce errors in diagnosis and classification.” The article states that machine learning algorithms evaluated by CAMELYON16, which evaluates how well algorithms can detect cancer, “[have] achieved encouraging results with a 92.4% sensitivity in tumor detection rate. In contrast, a pathology could only achieve 73.2% sensitivity.” Artificial intelligence is useful for the task of detecting cancer in pathology slide images because it can be trained to find patterns in data and is able to autonomously learn to solve novel problems. Currently, machine learning methods are being used to assist pathologic diagnosis by looking at cancer cells, cell nuclei, cell divisions, ducts, blood vessels, and more. Deep learning and artificial neural networks (ANNs) resemble human cognition, being composed of nodes (i.e. neurons), which make up an input layer, hidden layers, and an output layer. Convolutional neural networks (CNNs) are particularly adept at handling image classification tasks.\n",
    "\n",
    "The article Histopathological Cancer Detection with Deep Neural Networks [TowardsDataScience] discusses the use of deep neural networks for histopathological cancer detection. It explains how these advanced algorithms can analyze tissue samples to identify cancer cells with high accuracy. This article highlights the potential of deep neural networks in improving cancer diagnosis and treatment through automated precise detection methods.  \n",
    "\n",
    "The article The detection of cancer cells in histopathology based on machine vision [ScienceDirect-1], published July 20222 in Computers in Biology and Medicine, discusses why breast cancer detection is so important and how machine vision is able to improve traditional breast cancer detection. Breast cancer is an incredibly pressing issue facing the modern world. It is estimated that ⅛ women will develop breast cancer at some point, and by some metrics, it has overtaken lung leading cause of death out of all cancers worldwide. Furthermore, discovery in its early stages causes the possibility of a successful cure to rise to 80%. Traditional cancer detection through histopathological images is time consuming, difficult, hard to perform on large sets of data, and introduces a large amount of inconsistency from pathologist to pathologist. That’s why we are turning to machine image classification as it has the potential to be more accurate, more consistent, and much more efficient than individual pathologists could ever be.\n",
    "\n",
    "The article Current applications and challenges of artificial intelligence in pathology [ScienceDirect-2] explores the role of AI and ML in computational pathology. It discusses how AI algorithms can analyze medical images such as histopathology slides to assist in diagnosing diseases. The article emphasizes the potential of AI in improving accuracy, efficiency, and personalized medicine in pathology, while acknowledging the challenges and ethical considerations associated with its implementation. \n",
    "\n",
    "[Nature]: https://www.nature.com/articles/s41374-020-00514-0\n",
    "[TowardsDataScience]: https://towardsdatascience.com/histopathological-cancer-detection-with-deep-neural-networks-3399be879671 \n",
    "[ScienceDirect-1]: https://www.sciencedirect.com/science/article/pii/S0010482522004280 \n",
    "[ScienceDirect-2]: https://www.sciencedirect.com/science/article/pii/S2772736X22000081 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are facing is a binary classification problem: determining whether a 96x96 RGBimage is an instance of breast cancer or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our solution to this problem will be to train a convolutional neural network on the breast cancer image dataset to determine whether a tumor exists in the given image or not. Convolutional neural networks are typically applied to image classification tasks as they are able to learn features from the training images in service of the given task (binary classification). This means we won’t have to manually derive what pixels or groups of pixels in the image are indicative of a positive and negative class, the model will learn through gradient descent. We plan on building our own CNN model using PyTorch / PyTorch Lightning as well as loading a pre-trained ResNet50 model and retraining the last layer to map to our labels. Our solution will be tested by splitting our dataset into a training, validation, and test set. The training and validation set will be used during model training with the test set being used to report final evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "An evaluation metric we plan on using is the sensitivity and specificity. Breast cancer diagnosis is not something to be taken lightly so we want to make sure we’re choosing a threshold that balances false negative rate and false positive rate appropriately. It will also allow us to directly compare against different models. We also want to take into account the F-score and accuracy of our models to take into account all aspects of true and false diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We are using the PatchCamelyon ([PCam](https://patchcamelyon.grand-challenge.org/)) dataset, which was created by Bas Veeling and is used for training and benchmarking machine learning models. It contains 327,680 color images (96x96) of pathology slides with a binary classification that denotes the presence (labeled 1) or absence of cancer (labeled 0). The dataset is in the form of a .h5 file and consists of arrays with the corresponding pixel values (from 0 to 255) for each image.\n",
    "\n",
    "In order to load in the data, we first needed to unzip the files, since they came in a .gz format. Then, we had to read the .h5 files using their corresponding keys. Each .h5 file for the x_train, x_test, and x_valid data as well as the y_train, y_test, and y_valid data were loaded in as numpy arrays. The files with prefix “x” contained the pixel values for each image and the files with prefix “y” contained the binary labels for each image. After loading the files into memory, we were able to use a dataloader to pass the data into our model. We ran into problems on Datahub when trying to load all the files in at once, since we exceeded the storage limits, likely due to the gzipped x_train file and the unzipped x_train file taking up a combined 13.5gb. To counteract this, we unzipped the data locally and then uploaded the .h5 files to Datahub.\n",
    "\n",
    "Additional pre-processing steps we did to the data was min-max scaling (converting the pixel values from [0,255] to [0,1]), scaling from [0,255] to [-1,1], grayscaling all the data, as well as using a random vertical and horizontal flip scheme in the data loader (to reduce overfitting on the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "<img src=\"Models.png\"  width=\"500\">\n",
    "\n",
    "Starting with transfer learning, we imported many ImageNet pre-trained models from the PyTorch library. Replacing the last layer with a fully connected layer, we froze all other layers and trained just that singular layer. We tried different image sizes, batch sizes, optimisers, etc. These models yielded a wide variety of results, but none cracked the 80% test accuracy boundary, and seemed to stagnate after 5 or so epochs. Models such as ResNet50 are optimised for images of size 224x224 or larger, so images from this dataset, of size 96x96 (out of which only a center crop of 32x32 was supposed to be checked) did not perform well on the model. [Geert Litjens](https://geertlitjens.nl/post/getting-started-with-camelyon/), one of the publishers of this dataset, recommends a 6 layer convolutional, 3 layer dense, 2 layer dropout network, as a relatively shallow network equipped with dealing with small images. Following this strategy without using any transfer learning, we started getting relatively high training accuracies, breaking the 80% boundary. After trying over 50 models and parameter combinations over 3 weeks, we started fine tuning our model.\n",
    "\n",
    "### Model 1\n",
    "\n",
    "**Stochastic Gradient Descent** _(10 epochs, 5e-3 rate, 0.9 momentum, 0.4 dropout (2 dropout layers), 64 image batch size)_: 83.32% test accuracy\n",
    "\n",
    "<img src=\"model1.png\" width=\"300\">\n",
    "\n",
    "As seen, however, the validation accuracy does not decrease significantly after 3 epochs, indicating a level of overfitting on the training data. Although 83% is a good accuracy, it is likely this won't generalise well with similar pictures from different sets. This model is likely overfit on this dataset's colours, detail, etc. We decided to try more techniques to deal with this.\n",
    "\n",
    "### Model 2\n",
    "\n",
    "**Adaptive Momentum Estimation (Adam)** _(20 epochs, 1e-4 rate, 0.9 momentum, 0.3 dropout (3 dropout layers), 64 image batch size, random flips, grayscale, scheduler (1 patience, 0.1 factor) on validation accuracy)_: 81.89% test accuracy\n",
    "\n",
    "<img src=\"model3.png\" width=\"300\">\n",
    "\n",
    "Here, to deal with the overfitting, I introduced a third dropout layer (between the convolutional layer and the first dense layer), and introduced data augmentation. The first augmentation is random vertical and horizontal flipping of training data, effectively quadrupling the \"amount\" of training data to reduce overfitting. Secondly, I grayscaled the images to reduce overfitting on colour saturation. I carried out 20 epochs of Adam gradient descent, since the Adam optimiser is great at finding a minima regardless of training rate. I also introduced a scheduler at this point to fine tune the model. From this, it is clear to see how the Adam optimiser tends to overfit on training data, which is a known criticism of this optimiser. This test also showed that grayscale doesn't significantly reduce test accuracy.\n",
    "\n",
    "### Model 3\n",
    "\n",
    "**Stochastic Gradient Descent** _(30 epochs, 1e-3 rate, 0.92 momentum, 0.35 dropout (3 dropout layers), 32 image batch size, random flips, grayscale, scheduler (2 patience, 0.4 factor) on validation accuracy)_: 84.87% test accuracy\n",
    "\n",
    "<img src=\"model4.png\" width=\"300\">\n",
    "\n",
    "Learning from our previous models (which is discussed more in depth in the discussion section), we tweaked our training and retrained the model overnight. Giving us a fantastic, almost 85% accuracy, this model is incredibly accurate at finding the cancerous examples of patches, while not significantly overfitting on the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "We originally wanted to use a Naive-Bayes model to compare against our CNNs to see if we made discernable improvements on a basic model. A binary Naive-Bayes model just classifies based on the majority class in the dataset so we wanted to see if we could improve on chance accuracy. After discovering that the PCam dataset is actually fully balanced we opted for a logistic regression model straight from PyTorch which gave us a test accuracy of 65%. Our best test accuracy with model 3 reached 85% which is a 31% increase over our basic logistic model. This was a good baseline to test our model against.\n",
    "\n",
    "One of the main findings from this experiment was what elements of both our model and the dataset caused overfitting and led to less generalizability to unseen data. While we were testing different models we found that gray-scaling images helped with generalizability to the validation set as backed up by [experts](https://cs230.stanford.edu/projects_winter_2019/reports/15813329.pdf) in the field. We believe this is because we used a shallow network, and having too many variables can cause significant overfitting. Reducing to grayscale removes ⅔ of the information from the data, causing more generalization. \n",
    "\n",
    "\n",
    "With our second model we tried using stochastic gradient descent (SGD) vs adaptive momentum (Adam) to see how it affected our model’s generalization. We found that Adam caused it to overfit on our training data. Our validation loss stagnated while our training loss continued to decrease epoch after epoch. When researching online we found that Adam tends to overfit very fast on the training set and often requires early stopping criteria to maintain generalizability.\n",
    "\n",
    "From model 2 to model 3 we maintained the same CNN architecture slightly modifying the momentum values, dropout values, and scheduler. The main difference was reducing the batch size from 64 to 32 as well as training it for 10 additional epochs. In practice and while researching online we found that smaller batch sizes helps introduce more randomness into the training process which helps with generalizability. Small batches allow for finding real predictive features that are indicative of breast cancer rather than finding features that are specific to the current training batch.\n",
    "\n",
    "As far as our model’s performance goes, we found that testing the final model resulted in an accuracy of about 85.01%, a sensitivity/recall of about 77.7%, a specificity of about 92.3%, a false positive rate of about 7.7%, a false negative rate of about 22.3%, a precision of about 91.0%, and a F1-score of about 83.8%. Overall, these results are excellent, and means that our model was performing fairly well when generalizing to new data. Since sensitivity is calculated by TP/(TP + FN), this is one of the most important metrics for seeing how well our model detects cancer (i.e. higher is better). A sensitivity of 77.7% means that, out of all positives, we are detecting 77.7% of them correct, which means that we are properly diagnosing nearly 4 in 5 cases. According to the Nature publication “Artificial intelligence and computational pathology”, “pathologist could only achieve 73.2% sensitivity” ([source](https://www.nature.com/articles/s41374-020-00514-0)), so our model is performing better than the typical pathologist at detecting the presence of cancer.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "We ran into a few limitations with our work. One was not being able to try as many model combinations as we wanted. Due to limited GPU resources we weren’t able to use a grid search to try multiple different optimization methods, optimization values, and network architectures. We would train a model for an epoch or two and if it didn’t have impressive performance we’d scrap it to try a different combination instead. Another limitation we ran into was not being able to load the training, validation, and test set into memory at the same time. Because of this we weren’t able to set up an automatic system to produce complete metrics. One limitation we had with the dataset was the resolution of the images. 96x96 pixels is too small for ResNet50 to perform well on so we had to opt for different model architectures.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "There are a few ethical concerns that always arise using machine learning models. Firstly, the concern of explainability. Since a general practice in medicine is that one must always have full autonomy over their own data, using a machine learning model on that data to check if they have cancer or not can be a huge decision. Because of this, it’s considered good practice to make models explainable, so that users can use informed consent to opt-in or opt-out of certain algorithms being run on their data. Without understanding the ways that machine learning models work, people may not be able to give full informed consent to use their data, which brings up ethical concerns of the deployment of this model into the healthcare industry.\n",
    "\n",
    "Another common concern in reinforcement learning models (that this may be in deployment) is that of split consent. One may consent to use their data as an algorithm subject (such that their data is run through the model to get a response), but not as a data subject (such that their data is used to retrain the model). Especially in a field like medicine where biological records are considered PII data, having full control over such data is important. Further, with data consent laws in the state of California requiring the ability to revoke access to any of your data, it’s a question of how this model may be trained such that data is used in a manner that includes informed consent.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The development of a breast cancer classification model using deep learning techniques can have significant real world benefits which include assisting physicians in diagnosing patients with breast cancer and improving the accuracy or the assessment. This is useful so that patients can be prioritized for further examination. Ultimately the model has the potential of enhancing breast cancer diagnosis. It can also support clinical decision making to be more accurate, reliable, precise. We believe our model can significantly help with this goal, and we are getting scores similar to state-of-the-art models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    "In order to complete this project we had to go above and beyond what was taught in class in order to get a deeper understanding of the subject. What we have added on: \n",
    "\n",
    "- Research: We conducted extensive research in order to understand neural networks. This included loss functions, optimization algorithms, and regression functions. We also read papers about the effect of grayscaling, batch sizes, etc.\n",
    "- Exploring different Frameworks and Libraries:  We familiarized ourselves with deep learning frameworks and libraries such as Pytorch. Since some of us hadn't used these tools before for image processing, we had a lot of learning to do.\n",
    "- Data Collection and Preprocessing: We gathered relevant data for our project and performed preprocessing steps. It took a lot of work to get the data in a format ready to use, especially since it was an incredibly large dataset. Processing it on 8gb of RAM took days of effort.\n",
    "- Model Training: We experimented with different hyperparameters: batch size, learning rate, loss function, etc. We trained over 50 models to try this, which took 40+ hours."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
